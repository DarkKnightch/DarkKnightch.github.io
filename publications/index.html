<!DOCTYPE html><html lang="en" class="scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/19bf7477d82f620e.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-679b75d1c4c2027c.js"/><script src="/_next/static/chunks/4bd1b696-70b6d399998de86a.js" async=""></script><script src="/_next/static/chunks/684-5aaa8290a129f299.js" async=""></script><script src="/_next/static/chunks/main-app-a4a941c57aea68ed.js" async=""></script><script src="/_next/static/chunks/17-9c04c9413a9f9f1f.js" async=""></script><script src="/_next/static/chunks/874-6cc630662f3664af.js" async=""></script><script src="/_next/static/chunks/862-15038af301665bb3.js" async=""></script><script src="/_next/static/chunks/app/layout-6f80bd26db56b04f.js" async=""></script><script src="/_next/static/chunks/178-595a94b9af1e67b5.js" async=""></script><script src="/_next/static/chunks/748-1f3129a1e6365cf9.js" async=""></script><script src="/_next/static/chunks/app/%5Bslug%5D/page-1c9de6c76ad55e3b.js" async=""></script><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="dns-prefetch" href="https://google-fonts.jialeliu.com"/><link rel="preconnect" href="https://google-fonts.jialeliu.com" crossorigin=""/><link rel="preload" as="style" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/><link rel="stylesheet" id="gfonts-css" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap" media="print"/><script>
              (function(){
                var l = document.getElementById('gfonts-css');
                if (!l) return;
                if (l.media !== 'all') {
                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });
                }
              })();
            </script><noscript><link rel="stylesheet" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/></noscript><script>
              try {
                const theme = localStorage.getItem('theme-storage');
                const parsed = theme ? JSON.parse(theme) : null;
                const setting = parsed?.state?.theme || 'system';
                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));
                var root = document.documentElement;
                root.classList.add(effective);
                root.setAttribute('data-theme', effective);
              } catch (e) {
                var root = document.documentElement;
                root.classList.add('light');
                root.setAttribute('data-theme', 'light');
              }
            </script><title>Publications | Homepage of Siyuan Li</title><meta name="description" content="A collection of my research work."/><meta name="author" content="Siyuan Li"/><meta name="keywords" content="Siyuan Li,PhD,Research,Zhejiang University"/><meta name="creator" content="Siyuan Li"/><meta name="publisher" content="Siyuan Li"/><meta property="og:title" content="Homepage of Siyuan Li"/><meta property="og:description" content="PhD student at the Zhejiang University."/><meta property="og:site_name" content="Siyuan Li&#x27;s Academic Website"/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Homepage of Siyuan Li"/><meta name="twitter:description" content="PhD student at the Zhejiang University."/><link rel="icon" href="/favicon.svg"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans antialiased"><div style="visibility:hidden"><nav class="fixed top-0 left-0 right-0 z-50" data-headlessui-state=""><div class="transition-all duration-300 ease-out bg-transparent" style="transform:translateY(-100px)"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center h-16 lg:h-20"><div class="flex-shrink-0" tabindex="0"><a class="text-xl lg:text-2xl font-serif font-semibold text-primary hover:text-accent transition-colors duration-200" href="/">Homepage of Siyuan Li</a></div><div class="hidden lg:block"><div class="ml-10 flex items-center space-x-8"><div class="flex items-baseline space-x-8"><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/"><span class="relative z-10">About</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-primary" href="/publications/"><span class="relative z-10">Publications</span><div class="absolute inset-0 bg-accent/10 rounded-lg"></div></a></div><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div></div></div><div class="lg:hidden flex items-center space-x-2"><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div><button class="inline-flex items-center justify-center p-2 rounded-md text-neutral-600 hover:text-primary hover:bg-neutral-100 dark:hover:bg-neutral-800 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-accent transition-colors duration-200" id="headlessui-disclosure-button-Â«R5pdbÂ»" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open main menu</span><div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="block h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></div></button></div></div></div></div></nav><main class="min-h-screen pt-16 lg:pt-20"><div class="max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-12"><div style="opacity:0;transform:translateY(20px)"><div class="mb-8"><h1 class="text-4xl font-serif font-bold text-primary mb-4">Publications</h1><p class="text-lg text-neutral-600 dark:text-neutral-500 max-w-2xl">A collection of my research work.</p></div><div class="mb-8 space-y-4"><div class="flex flex-col sm:flex-row gap-4"><div class="relative flex-grow"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="absolute left-3 top-1/2 transform -translate-y-1/2 h-5 w-5 text-neutral-400"><path stroke-linecap="round" stroke-linejoin="round" d="m21 21-5.197-5.197m0 0A7.5 7.5 0 1 0 5.196 5.196a7.5 7.5 0 0 0 10.607 10.607Z"></path></svg><input type="text" placeholder="Search publications..." class="w-full pl-10 pr-4 py-2 rounded-lg border border-neutral-200 dark:border-neutral-800 bg-white dark:bg-neutral-900 focus:ring-2 focus:ring-accent focus:border-transparent transition-all duration-200" value=""/></div><button class="flex items-center justify-center px-4 py-2 rounded-lg border transition-all duration-200 bg-white dark:bg-neutral-900 border-neutral-200 dark:border-neutral-800 text-neutral-600 hover:border-accent hover:text-accent"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5 mr-2"><path stroke-linecap="round" stroke-linejoin="round" d="M12 3c2.755 0 5.455.232 8.083.678.533.09.917.556.917 1.096v1.044a2.25 2.25 0 0 1-.659 1.591l-5.432 5.432a2.25 2.25 0 0 0-.659 1.591v2.927a2.25 2.25 0 0 1-1.244 2.013L9.75 21v-6.568a2.25 2.25 0 0 0-.659-1.591L3.659 7.409A2.25 2.25 0 0 1 3 5.818V4.774c0-.54.384-1.006.917-1.096A48.32 48.32 0 0 1 12 3Z"></path></svg>Filters</button></div></div><div class="space-y-6"><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Basis Function Learning for Variable-Length and Continuous-Indexed Signals" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/CBFL.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Basis Function Learning for Variable-Length and Continuous-Indexed Signals</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Siyuan Li</span>, </span><span><span class="">Lei Cheng</span>, </span><span><span class="">Feng Yin</span>, </span><span><span class="">Jianlong Li</span>, </span><span><span class="">Peter Gerstoft</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)<!-- --> <!-- -->2025</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">This paper introducs a Bayesian functional representation model to facilitate interpretable and effective basis function learning.</p><div class="flex flex-wrap gap-2 mt-auto"><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Learning data distribution of three-dimensional ocean sound speed fields via diffusion models" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/DiffusionSSF.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Learning data distribution of three-dimensional ocean sound speed fields via diffusion models</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Siyuan Li</span>, </span><span><span class="">Lei Cheng</span>, </span><span><span class="">Jun Li</span>, </span><span><span class="">Zichen Wang</span>, </span><span><span class="">Jianlong Li</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">The Journal of the Acoustical Society of America<!-- --> <!-- -->2024</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">This paper presents the first evaluation of the diffusion model&#x27;s effectiveness in generating 3D SSF data.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/https://doi.org/10.1121/10.0026026" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/OceanSTARLab/DiffusionSSF" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Zero-shot reconstruction of ocean sound speed field tensors: A deep plug-and-play approach" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/Deeppnp.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Zero-shot reconstruction of ocean sound speed field tensors: A deep plug-and-play approach</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Siyuan Li</span>, </span><span><span class="">Lei Cheng</span>, </span><span><span class="">Xiao Fu</span>, </span><span><span class="">Jianlong Li</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">The Journal of the Acoustical Society of America<!-- --> <!-- -->2024</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">This paper proposes a tailored plug-and-play framework with image denoiser for SSF recovery.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/https://doi.org/10.1121/10.0026125" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/OceanSTARLab/DeepPnP" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Striking the right balance: Three-dimensional ocean sound speed field reconstruction using tensor neural networks" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/TNN.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Striking the right balance: Three-dimensional ocean sound speed field reconstruction using tensor neural networks</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Siyuan Li</span>, </span><span><span class="">Lei Cheng</span>, </span><span><span class="">Ting Zhang</span>, </span><span><span class="">Hangfang Zhao</span>, </span><span><span class="">Jianlong Li</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">The Journal of the Acoustical Society of America<!-- --> <!-- -->2023</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">This paper proposes a 3D SSF-tailored tensor DNN is proposed, which uses tensor computations and DNN architectures to achieve remarkable 3D SSF reconstruction.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/https://doi.org/10.1121/10.0020670" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/OceanSTARLab/Tensor-Neural-Network" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Graph-guided Bayesian matrix completion for ocean sound speed field reconstruction" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/BMCG.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Graph-guided Bayesian matrix completion for ocean sound speed field reconstruction</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Siyuan Li</span>, </span><span><span class="">Lei Cheng</span>, </span><span><span class="">Ting Zhang</span>, </span><span><span class="">Hangfang Zhao</span>, </span><span><span class="">Jianlong Li</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">The Journal of the Acoustical Society of America<!-- --> <!-- -->2023</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">This paper introduces graph-guided Bayesian low-rank matrix completions (LRMCs) for fine-scale and accurate ocean SSF reconstruction.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/https://doi.org/10.1121/10.0017064" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/OceanSTARLab/Graph-guided-Bayesian-Matrix-Completion" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div></div></div></div><!--$--><!--/$--><!--$--><!--/$--></main><footer class="border-t border-neutral-200/50 bg-neutral-50/50 dark:bg-neutral-900/50 dark:border-neutral-700/50"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-6"><div class="flex flex-col sm:flex-row justify-between items-center gap-2"><p class="text-xs text-neutral-500">Last updated: <!-- -->November 27, 2025</p><p class="text-xs text-neutral-500 flex items-center"><a href="https://github.com/xyjoey/PRISM" target="_blank" rel="noopener noreferrer">Built with PRISM</a><span class="ml-2">ðŸš€</span></p></div></div></footer></div><script src="/_next/static/chunks/webpack-679b75d1c4c2027c.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[3719,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-6f80bd26db56b04f.js\"],\"ThemeProvider\"]\n3:I[768,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-6f80bd26db56b04f.js\"],\"default\"]\n4:I[7555,[],\"\"]\n5:I[1295,[],\"\"]\n6:I[2548,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-6f80bd26db56b04f.js\"],\"default\"]\n8:I[9665,[],\"MetadataBoundary\"]\na:I[9665,[],\"OutletBoundary\"]\nd:I[4911,[],\"AsyncMetadataOutlet\"]\nf:I[9665,[],\"ViewportBoundary\"]\n11:I[6614,[],\"\"]\n:HL[\"/_next/static/css/19bf7477d82f620e.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"-3Ayk5h3O4Dnmwy5nUMGe\",\"p\":\"\",\"c\":[\"\",\"publications\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[[\"slug\",\"publications\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/19bf7477d82f620e.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/favicon.svg\",\"type\":\"image/svg+xml\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://google-fonts.jialeliu.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://google-fonts.jialeliu.com\",\"crossOrigin\":\"\"}],[\"$\",\"link\",null,{\"rel\":\"preload\",\"as\":\"style\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}],[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"id\":\"gfonts-css\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\",\"media\":\"print\"}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              (function(){\\n                var l = document.getElementById('gfonts-css');\\n                if (!l) return;\\n                if (l.media !== 'all') {\\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\\n                }\\n              })();\\n            \"}}],[\"$\",\"noscript\",null,{\"children\":[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}]}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              try {\\n                const theme = localStorage.getItem('theme-storage');\\n                const parsed = theme ? JSON.parse(theme) : null;\\n                const setting = parsed?.state?.theme || 'system';\\n                const prefersDark = typeof window !== 'undefined' \u0026\u0026 window.matchMedia \u0026\u0026 window.matchMedia('(prefers-color-scheme: dark)').matches;\\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\\n                var root = document.documentElement;\\n                root.classList.add(effective);\\n                root.setAttribute('data-theme', effective);\\n              } catch (e) {\\n                var root = document.documentElement;\\n                root.classList.add('light');\\n                root.setAttribute('data-theme', 'light');\\n              }\\n            \"}}]]}],[\"$\",\"body\",null,{\"className\":\"font-sans antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[[\"$\",\"$L3\",null,{\"items\":[{\"title\":\"About\",\"type\":\"page\",\"target\":\"about\",\"href\":\"/\"},{\"title\":\"Publications\",\"type\":\"page\",\"target\":\"publications\",\"href\":\"/publications\"}],\"siteTitle\":\"Homepage of Siyuan Li\",\"enableOnePageMode\":false}],[\"$\",\"main\",null,{\"className\":\"min-h-screen pt-16 lg:pt-20\",\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"$L6\",null,{\"lastUpdated\":\"November 27, 2025\"}]]}]}]]}]]}],{\"children\":[[\"slug\",\"publications\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L7\",[\"$\",\"$L8\",null,{\"children\":\"$L9\"}],null,[\"$\",\"$La\",null,{\"children\":[\"$Lb\",\"$Lc\",[\"$\",\"$Ld\",null,{\"promise\":\"$@e\"}]]}]]}],{},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"XHlBKMcowPkJt0w-asjI8\",{\"children\":[[\"$\",\"$Lf\",null,{\"children\":\"$L10\"}],null]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$11\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"12:\"$Sreact.suspense\"\n13:I[4911,[],\"AsyncMetadata\"]\n15:I[6669,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"178\",\"static/chunks/178-595a94b9af1e67b5.js\",\"748\",\"static/chunks/748-1f3129a1e6365cf9.js\",\"182\",\"static/chunks/app/%5Bslug%5D/page-1c9de6c76ad55e3b.js\"],\"default\"]\n9:[\"$\",\"$12\",null,{\"fallback\":null,\"children\":[\"$\",\"$L13\",null,{\"promise\":\"$@14\"}]}]\n16:T507,@inproceedings{li2025basis,\n  title = {Basis Function Learning for Variable-Length and Continuous-Indexed Signals},\n  author = {Li, Siyuan and Cheng, Lei and Yin, Feng and Li, Jianlong and Gerstoft, Peter},\n  booktitle = {ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n  pages = {1--5},\n  year = {2025},\n  organization = {IEEE},\n  abstract = {Representing variable-length and continuous-indexed signals through a linear combination of basis functions poses a fundamental challenge in science and engineering. Current approaches resort to preprocessing steps, such as interpolation and extrapolation, to handle irregular and off-grid measurements, which compromise the physical nature of signals and degrade the representation performance. To address this challenge, rather than utilizing discrete vectors, we introduce a Bayesian functional representation model that capitalizes on the continuous nature and rich expressiveness of Gaussian processes to facilitate interpretable and effective basis function learning. Moreover, an analytical and efficient algorithm based on the variational inference framework is developed. Experimental results using real-life datasets demonstrate the superior performance of our proposed method.}\n}17:T692,The probability distribution of three-dimensional sound speed fields (3D SSFs) in an ocean region encapsulates vital information about their variations, serving as valuable data-driven priors for SSF inversion tasks. However, learning such a distribution is challenging due to the high dimensionality and complexity of 3D SSFs. To tackle this challenge, we propose employing the diffusio"])</script><script>self.__next_f.push([1,"n model, a cutting-edge deep generative model that has showcased remarkable performance in diverse domains, including image and audio processing. Nonetheless, applying this approach to 3D ocean SSFs encounters two primary hurdles. First, the lack of publicly available well-crafted 3D SSF datasets impedes training and evaluation. Second, 3D SSF data consist of multiple 2D layers with varying variances, which can lead to uneven denoising during the reverse process. To surmount these obstacles, we introduce a novel 3D SSF dataset called 3DSSF, specifically designed for training and evaluating deep generative models. In addition, we devise a high-capacity neural architecture for the diffusion model to effectively handle variations in 3D sound speeds. Furthermore, we employ state-of-the-art continuous-time-based optimization method and predictor-corrector scheme for high-performance training and sampling. Notably, this paper presents the first evaluation of the diffusion model's effectiveness in generating 3D SSF data. Numerical experiments validate the proposed method's strong ability to learn the underlying data distribution of 3D SSFs, and highlight its effectiveness in assisting SSF inversion tasks and subsequently characterizing the transmission loss of underwater acoustics.18:T856,"])</script><script>self.__next_f.push([1,"@article{li2024learning,\n  title = {Learning data distribution of three-dimensional ocean sound speed fields via diffusion models},\n  author = {Li, Siyuan and Cheng, Lei and Li, Jun and Wang, Zichen and Li, Jianlong},\n  journal = {The Journal of the Acoustical Society of America},\n  volume = {155},\n  number = {5},\n  pages = {3410--3425},\n  year = {2024},\n  publisher = {AIP Publishing},\n  abstract = {The probability distribution of three-dimensional sound speed fields (3D SSFs) in an ocean region encapsulates vital information about their variations, serving as valuable data-driven priors for SSF inversion tasks. However, learning such a distribution is challenging due to the high dimensionality and complexity of 3D SSFs. To tackle this challenge, we propose employing the diffusion model, a cutting-edge deep generative model that has showcased remarkable performance in diverse domains, including image and audio processing. Nonetheless, applying this approach to 3D ocean SSFs encounters two primary hurdles. First, the lack of publicly available well-crafted 3D SSF datasets impedes training and evaluation. Second, 3D SSF data consist of multiple 2D layers with varying variances, which can lead to uneven denoising during the reverse process. To surmount these obstacles, we introduce a novel 3D SSF dataset called 3DSSF, specifically designed for training and evaluating deep generative models. In addition, we devise a high-capacity neural architecture for the diffusion model to effectively handle variations in 3D sound speeds. Furthermore, we employ state-of-the-art continuous-time-based optimization method and predictor-corrector scheme for high-performance training and sampling. Notably, this paper presents the first evaluation of the diffusion model's effectiveness in generating 3D SSF data. Numerical experiments validate the proposed method's strong ability to learn the underlying data distribution of 3D SSFs, and highlight its effectiveness in assisting SSF inversion tasks and subsequently characterizing the transmission loss of underwater acoustics.},\n  doi = {https://doi.org/10.1121/10.0026026}\n}"])</script><script>self.__next_f.push([1,"19:T5fb,Reconstructing a three-dimensional ocean sound speed field (SSF) from limited and noisy measurements presents an ill-posed and challenging inverse problem. Existing methods used a number of pre-specified priors (e.g., low-rank tensor and tensor neural network structures) to address this issue. However, the SSFs are often too complex to be accurately described by these pre-defined priors. While utilizing neural network-based priors trained on historical SSF data may be a viable workaround, acquiring SSF data remains a nontrivial task. This work starts with a key observation: Although natural images and SSFs admit fairly different characteristics, their denoising processes appear to share similar traitsâ€”as both remove random components from more structured signals. This observation allows us to incorporate deep denoisers trained using extensive natural images to realize zero-shot SSF reconstruction, without any extra training or network modifications. To implement this idea, an alternating direction method of multipliers (ADMM) algorithm using such a deep denoiser is proposed, which is reminiscent of the plug-and-play scheme from medical imaging. Our plug-and-play framework is tailored for SSF recovery such that the learned denoiser can be simultaneously used with other handcrafted SSF priors. Extensive numerical studies show that the new framework largely outperforms state-of-the-art baselines, especially under widely recognized challenging scenarios, e.g., when the SSF samples are taken as tensor fibers.1a:T7a8,@article{li2024zero,\n  title = {Zero-shot reconstruction of ocean sound speed field tensors: A deep plug-and-play approach},\n  author = {Li, Siyuan and Cheng, Lei and Fu, Xiao and Li, Jianlong},\n  journal = {The Journal of the Acoustical Society of America},\n  volume = {155},\n  number = {5},\n  pages = {3475--3489},\n  year = {2024},\n  publisher = {AIP Publishing},\n  abstract = {Reconstructing a three-dimensional ocean sound speed field (SSF) from limited and noisy measurements presents an ill-pose"])</script><script>self.__next_f.push([1,"d and challenging inverse problem. Existing methods used a number of pre-specified priors (e.g., low-rank tensor and tensor neural network structures) to address this issue. However, the SSFs are often too complex to be accurately described by these pre-defined priors. While utilizing neural network-based priors trained on historical SSF data may be a viable workaround, acquiring SSF data remains a nontrivial task. This work starts with a key observation: Although natural images and SSFs admit fairly different characteristics, their denoising processes appear to share similar traitsâ€”as both remove random components from more structured signals. This observation allows us to incorporate deep denoisers trained using extensive natural images to realize zero-shot SSF reconstruction, without any extra training or network modifications. To implement this idea, an alternating direction method of multipliers (ADMM) algorithm using such a deep denoiser is proposed, which is reminiscent of the plug-and-play scheme from medical imaging. Our plug-and-play framework is tailored for SSF recovery such that the learned denoiser can be simultaneously used with other handcrafted SSF priors. Extensive numerical studies show that the new framework largely outperforms state-of-the-art baselines, especially under widely recognized challenging scenarios, e.g., when the SSF samples are taken as tensor fibers.},\n  doi = {https://doi.org/10.1121/10.0026125}\n}1b:T49b,Accurately reconstructing a three-dimensional (3D) ocean sound speed field (SSF) is essential for various ocean acoustic applications, but the sparsity and uncertainty of sound speed samples across a vast ocean region make it a challenging task. To tackle this challenge, a large body of reconstruction methods has been developed, including spline interpolation, matrix or tensor-based completion, and deep neural networks (DNNs)-based reconstruction. However, a principled analysis of their effectiveness in 3D SSF reconstruction is still lacking. This paper performs a thorough a"])</script><script>self.__next_f.push([1,"nalysis of the reconstruction error and highlights the need for a balanced representation model that integrates expressiveness and conciseness. To meet this requirement, a 3D SSF-tailored tensor DNN is proposed, which uses tensor computations and DNN architectures to achieve remarkable 3D SSF reconstruction. The proposed model not only includes the previous tensor-based SSF representation model as a special case but also has a natural ability to reject noise. The numerical results using the South China Sea 3D SSF data demonstrate that the proposed method outperforms state-of-the-art methods.1c:T679,@article{li2023striking,\n  title = {Striking the right balance: Three-dimensional ocean sound speed field reconstruction using tensor neural networks},\n  author = {Li, Siyuan and Cheng, Lei and Zhang, Ting and Zhao, Hangfang and Li, Jianlong},\n  journal = {The Journal of the Acoustical Society of America},\n  volume = {154},\n  number = {2},\n  pages = {1106--1123},\n  year = {2023},\n  publisher = {AIP Publishing},\n  abstract = {Accurately reconstructing a three-dimensional (3D) ocean sound speed field (SSF) is essential for various ocean acoustic applications, but the sparsity and uncertainty of sound speed samples across a vast ocean region make it a challenging task. To tackle this challenge, a large body of reconstruction methods has been developed, including spline interpolation, matrix or tensor-based completion, and deep neural networks (DNNs)-based reconstruction. However, a principled analysis of their effectiveness in 3D SSF reconstruction is still lacking. This paper performs a thorough analysis of the reconstruction error and highlights the need for a balanced representation model that integrates expressiveness and conciseness. To meet this requirement, a 3D SSF-tailored tensor DNN is proposed, which uses tensor computations and DNN architectures to achieve remarkable 3D SSF reconstruction. The proposed model not only includes the previous tensor-based SSF representation model as a special case but also has a n"])</script><script>self.__next_f.push([1,"atural ability to reject noise. The numerical results using the South China Sea 3D SSF data demonstrate that the proposed method outperforms state-of-the-art methods.},\n  doi = {https://doi.org/10.1121/10.0020670}\n}1d:T60e,Reconstructing ocean sound speed field (SSF) from limited and noisy measurements/estimates is crucial for many ocean acoustic applications, including underwater tomography, target localization/tracking, and communications. Classical reconstruction methods include deterministic approaches (e.g., spline interpolation) and geostatistical methods (e.g., kriging). They exhibit a strong link to linear regression and Gaussian process regression in machine learning (ML) literature, by uniformly viewing them as supervised regression models that learn the mapping from the geographical locations to the sound speed outputs. From a unified ML perspective, theoretical analysis indicates that classical reconstruction methods have several drawbacks, such as the sensitivity to noises and high computational cost. To overcome these drawbacks, inspired by the recent thriving development of graph machine learning, we introduce graph-guided Bayesian low-rank matrix completions (LRMCs) for fine-scale and accurate ocean SSF reconstruction. In particular, a more general graph-guided LRMC model is proposed that encompasses the state-of-the-art one as a special case. The proposed model and the associated inference algorithm simultaneously exploit the global (low-rankness) and local (graph structure) information of ocean sound speed data, thus striking an outstanding balance of reconstruction accuracy and computational complexity. Numerical results using real-life ocean SSF data have demonstrated the encouraging performances of the proposed approaches.1e:T7c8,@article{li2023graph,\n  title = {Graph-guided Bayesian matrix completion for ocean sound speed field reconstruction},\n  author = {Li, Siyuan and Cheng, Lei and Zhang, Ting and Zhao, Hangfang and Li, Jianlong},\n  journal = {The Journal of the Acoustical Society of Ame"])</script><script>self.__next_f.push([1,"rica},\n  volume = {153},\n  number = {1},\n  pages = {689--710},\n  year = {2023},\n  publisher = {AIP Publishing},\n  abstract = {Reconstructing ocean sound speed field (SSF) from limited and noisy measurements/estimates is crucial for many ocean acoustic applications, including underwater tomography, target localization/tracking, and communications. Classical reconstruction methods include deterministic approaches (e.g., spline interpolation) and geostatistical methods (e.g., kriging). They exhibit a strong link to linear regression and Gaussian process regression in machine learning (ML) literature, by uniformly viewing them as supervised regression models that learn the mapping from the geographical locations to the sound speed outputs. From a unified ML perspective, theoretical analysis indicates that classical reconstruction methods have several drawbacks, such as the sensitivity to noises and high computational cost. To overcome these drawbacks, inspired by the recent thriving development of graph machine learning, we introduce graph-guided Bayesian low-rank matrix completions (LRMCs) for fine-scale and accurate ocean SSF reconstruction. In particular, a more general graph-guided LRMC model is proposed that encompasses the state-of-the-art one as a special case. The proposed model and the associated inference algorithm simultaneously exploit the global (low-rankness) and local (graph structure) information of ocean sound speed data, thus striking an outstanding balance of reconstruction accuracy and computational complexity. Numerical results using real-life ocean SSF data have demonstrated the encouraging performances of the proposed approaches.},\n  doi = {https://doi.org/10.1121/10.0017064}\n}"])</script><script>self.__next_f.push([1,"7:[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-12\",\"children\":[[\"$\",\"$L15\",null,{\"config\":{\"type\":\"publication\",\"title\":\"Publications\",\"description\":\"A collection of my research work.\",\"source\":\"publications.bib\"},\"publications\":[{\"id\":\"li2025basis\",\"title\":\"Basis Function Learning for Variable-Length and Continuous-Indexed Signals\",\"authors\":[{\"name\":\"Siyuan Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Lei Cheng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Feng Yin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jianlong Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Peter Gerstoft\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:0:tags\",\"researchArea\":\"signal-processing\",\"journal\":\"\",\"conference\":\"ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"pages\":\"1--5\",\"abstract\":\"Representing variable-length and continuous-indexed signals through a linear combination of basis functions poses a fundamental challenge in science and engineering. Current approaches resort to preprocessing steps, such as interpolation and extrapolation, to handle irregular and off-grid measurements, which compromise the physical nature of signals and degrade the representation performance. To address this challenge, rather than utilizing discrete vectors, we introduce a Bayesian functional representation model that capitalizes on the continuous nature and rich expressiveness of Gaussian processes to facilitate interpretable and effective basis function learning. Moreover, an analytical and efficient algorithm based on the variational inference framework is developed. Experimental results using real-life datasets demonstrate the superior performance of our proposed method.\",\"description\":\"This paper introducs a Bayesian functional representation model to facilitate interpretable and effective basis function learning.\",\"selected\":true,\"preview\":\"CBFL.png\",\"bibtex\":\"$16\"},{\"id\":\"li2024learning\",\"title\":\"Learning data distribution of three-dimensional ocean sound speed fields via diffusion models\",\"authors\":[{\"name\":\"Siyuan Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Lei Cheng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jun Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zichen Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jianlong Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:1:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"The Journal of the Acoustical Society of America\",\"conference\":\"\",\"volume\":\"155\",\"issue\":\"5\",\"pages\":\"3410--3425\",\"doi\":\"https://doi.org/10.1121/10.0026026\",\"code\":\"https://github.com/OceanSTARLab/DiffusionSSF\",\"abstract\":\"$17\",\"description\":\"This paper presents the first evaluation of the diffusion model's effectiveness in generating 3D SSF data.\",\"selected\":true,\"preview\":\"DiffusionSSF.png\",\"bibtex\":\"$18\"},{\"id\":\"li2024zero\",\"title\":\"Zero-shot reconstruction of ocean sound speed field tensors: A deep plug-and-play approach\",\"authors\":[{\"name\":\"Siyuan Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Lei Cheng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiao Fu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jianlong Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:2:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"The Journal of the Acoustical Society of America\",\"conference\":\"\",\"volume\":\"155\",\"issue\":\"5\",\"pages\":\"3475--3489\",\"doi\":\"https://doi.org/10.1121/10.0026125\",\"code\":\"https://github.com/OceanSTARLab/DeepPnP\",\"abstract\":\"$19\",\"description\":\"This paper proposes a tailored plug-and-play framework with image denoiser for SSF recovery.\",\"selected\":false,\"preview\":\"Deeppnp.png\",\"bibtex\":\"$1a\"},{\"id\":\"li2023striking\",\"title\":\"Striking the right balance: Three-dimensional ocean sound speed field reconstruction using tensor neural networks\",\"authors\":[{\"name\":\"Siyuan Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Lei Cheng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ting Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hangfang Zhao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jianlong Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2023,\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"Tensor decomposotion\",\"Neural network\",\"Inverse problem\"],\"keywords\":\"$7:props:children:0:props:publications:3:tags\",\"researchArea\":\"neural-networks\",\"journal\":\"The Journal of the Acoustical Society of America\",\"conference\":\"\",\"volume\":\"154\",\"issue\":\"2\",\"pages\":\"1106--1123\",\"doi\":\"https://doi.org/10.1121/10.0020670\",\"code\":\"https://github.com/OceanSTARLab/Tensor-Neural-Network\",\"abstract\":\"$1b\",\"description\":\"This paper proposes a 3D SSF-tailored tensor DNN is proposed, which uses tensor computations and DNN architectures to achieve remarkable 3D SSF reconstruction.\",\"selected\":true,\"preview\":\"TNN.png\",\"bibtex\":\"$1c\"},{\"id\":\"li2023graph\",\"title\":\"Graph-guided Bayesian matrix completion for ocean sound speed field reconstruction\",\"authors\":[{\"name\":\"Siyuan Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Lei Cheng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ting Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hangfang Zhao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jianlong Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2023,\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:4:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"The Journal of the Acoustical Society of America\",\"conference\":\"\",\"volume\":\"153\",\"issue\":\"1\",\"pages\":\"689--710\",\"doi\":\"https://doi.org/10.1121/10.0017064\",\"code\":\"https://github.com/OceanSTARLab/Graph-guided-Bayesian-Matrix-Completion\",\"abstract\":\"$1d\",\"description\":\"This paper introduces graph-guided Bayesian low-rank matrix completions (LRMCs) for fine-scale and accurate ocean SSF reconstruction.\",\"selected\":false,\"preview\":\"BMCG.png\",\"bibtex\":\"$1e\"}]}],false,false]}]\n"])</script><script>self.__next_f.push([1,"c:null\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\nb:null\n"])</script><script>self.__next_f.push([1,"14:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Publications | Homepage of Siyuan Li\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"A collection of my research work.\"}],[\"$\",\"meta\",\"2\",{\"name\":\"author\",\"content\":\"Siyuan Li\"}],[\"$\",\"meta\",\"3\",{\"name\":\"keywords\",\"content\":\"Siyuan Li,PhD,Research,Zhejiang University\"}],[\"$\",\"meta\",\"4\",{\"name\":\"creator\",\"content\":\"Siyuan Li\"}],[\"$\",\"meta\",\"5\",{\"name\":\"publisher\",\"content\":\"Siyuan Li\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"Homepage of Siyuan Li\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"PhD student at the Zhejiang University.\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:site_name\",\"content\":\"Siyuan Li's Academic Website\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:title\",\"content\":\"Homepage of Siyuan Li\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:description\",\"content\":\"PhD student at the Zhejiang University.\"}],[\"$\",\"link\",\"14\",{\"rel\":\"icon\",\"href\":\"/favicon.svg\"}]],\"error\":null,\"digest\":\"$undefined\"}\ne:{\"metadata\":\"$14:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>